#!/usr/bin/env python3
"""
Pipeline OptimisÃ© v2 - Conservative Approach
Garder 50.66% baseline et ajouter + optimisations lÃ©gÃ¨res
"""

import time
import pandas as pd
import numpy as np
from pathlib import Path
import lightgbm as lgbm
from sklearn.metrics import accuracy_score

from config import *
from data_loading import load_data


def print_header(title):
    print("\n" + "="*60)
    print(f"  {title}")
    print("="*60)


def print_step(step_num, description):
    print(f"\nğŸ“ Step {step_num}: {description}")
    print("-" * 60)


def train_lgbm_model(X_train, y_train, params, num_rounds=300):
    """EntraÃ®ne un modÃ¨le LightGBM"""
    train_data = lgbm.Dataset(X_train, label=y_train)
    model = lgbm.train(params, train_data, num_boost_round=num_rounds)
    return model


def main():
    """Pipeline conservateur : 50.66% baseline + optimisations lÃ©gÃ¨res"""
    
    print_header("ğŸš€ PIPELINE OPTIMISÃ‰ v2 - Conservative")
    print(f"ğŸ“Š Objectif: Garder 50.66% et +0.5%")
    print(f"â° DÃ©marrage: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    start_time = time.time()
    
    # ========================================
    # Ã‰TAPE 1: Chargement
    # ========================================
    print_step(1, "Chargement des donnÃ©es")
    
    X_train, y_train, X_test, sample_submission = load_data()
    print(f"âœ“ X_train: {X_train.shape}")
    print(f"âœ“ y_train: {y_train.shape}")
    print(f"âœ“ X_test: {X_test.shape}")
    
    # Nettoyage colonnes non-numÃ©riques
    if hasattr(X_train, 'select_dtypes'):
        numeric_cols = X_train.select_dtypes(include=['float64', 'int64', 'float32', 'int32']).columns
        X_train = X_train[numeric_cols].copy()
        X_test = X_test[numeric_cols].copy()
        print(f"âœ“ X_train nettoyÃ©: {X_train.shape}")
    
    # Convertir en numpy
    if hasattr(X_train, 'values'):
        X_train = X_train.values
    if hasattr(X_test, 'values'):
        X_test = X_test.values
    if hasattr(y_train, 'values'):
        y_train = y_train.values.flatten()
    
    X_train = X_train.astype(np.float32)
    X_test = X_test.astype(np.float32)
    y_train = y_train.astype(np.int32)
    
    print(f"âœ“ X_train {X_train.shape}, y_train {y_train.shape}")
    
    # ========================================
    # Ã‰TAPE 2: Split train/validation
    # ========================================
    print_step(2, "Split train/validation (80/20)")
    
    n_split = int(0.8 * len(X_train))
    X_train_split, X_val_split = X_train[:n_split], X_train[n_split:]
    y_train_split, y_val_split = y_train[:n_split], y_train[n_split:]
    
    print(f"âœ“ Train: {X_train_split.shape}, Val: {X_val_split.shape}")
    
    # ========================================
    # Ã‰TAPE 3: Hyperparameter Tuning SIMPLE
    # ========================================
    print_step(3, "Hyperparameter Tuning (2 configs seulement)")
    
    print("âš¡ TEST 2 CONFIGURATIONS\n")
    
    tested_params = [
        # Config 1: Baseline original (50.66% known)
        {
            "learning_rate": 0.05,
            "max_depth": 5,
            "num_leaves": 31,
            "subsample": 0.8,
            "colsample_bytree": 0.8,
        },
        # Config 2: LÃ©ger ajustement
        {
            "learning_rate": 0.04,
            "max_depth": 5,
            "num_leaves": 31,
            "subsample": 0.85,
            "colsample_bytree": 0.85,
        },
    ]
    
    best_params = None
    best_accuracy = 0
    all_models = []
    
    for i, params in enumerate(tested_params, 1):
        full_params = {
            "objective": "binary",
            "metric": "binary_logloss",
            "num_threads": 1,
            "seed": 42,
            "verbosity": -1,
            **params
        }
        
        model = train_lgbm_model(X_train_split, y_train_split, full_params)
        all_models.append(model)
        
        y_val_pred = (model.predict(X_val_split) > 0.5).astype(int)
        accuracy = accuracy_score(y_val_split, y_val_pred)
        
        print(f"   Config {i}: Accuracy = {accuracy:.4f}")
        
        if accuracy > best_accuracy:
            best_accuracy = accuracy
            best_params = full_params
    
    print(f"\nâœ¨ Meilleure config: {best_accuracy:.4f}")
    
    # ========================================
    # Ã‰TAPE 4: EntraÃ®ner modÃ¨le final avec les meilleurs params
    # ========================================
    print_step(4, "EntraÃ®nement modÃ¨le final")
    
    lgbm_final = train_lgbm_model(X_train, y_train, best_params, num_rounds=300)
    print(f"âœ“ ModÃ¨le LightGBM entraÃ®nÃ©")
    
    # ========================================
    # Ã‰TAPE 5: Ensemble simple (moyenne des modÃ¨les)
    # ========================================
    print_step(5, "Simple Ensemble (Moyenne)")
    
    # Moyenne des prÃ©dictions (plus robuste que stacking complexe)
    y_pred_model1 = all_models[0].predict(X_test)
    y_pred_model2 = all_models[1].predict(X_test)
    y_pred_final = lgbm_final.predict(X_test)
    
    # Moyenne simple (poids Ã©gaux)
    y_pred_avg = (y_pred_model1 + y_pred_model2 + y_pred_final) / 3
    
    print(f"âœ“ Ensemble: moyenne de 3 modÃ¨les")
    print(f"âœ“ PrÃ©dictions avg: min={y_pred_avg.min():.4f}, max={y_pred_avg.max():.4f}")
    
    # ========================================
    # Ã‰TAPE 6: Threshold Optimization (SIMPLE)
    # ========================================
    print_step(6, "Threshold Optimization")
    
    # Utiliser le modÃ¨le final simple pour threshold
    y_val_pred_final = lgbm_final.predict(X_val_split)
    
    best_threshold = 0.5
    best_val_accuracy = 0
    
    print("  Test des seuils sur validation:")
    for threshold in np.arange(0.40, 0.60, 0.02):
        y_pred_thresh = (y_val_pred_final > threshold).astype(int)
        acc = accuracy_score(y_val_split, y_pred_thresh)
        print(f"    Threshold {threshold:.2f}: Accuracy = {acc:.4f}")
        if acc > best_val_accuracy:
            best_val_accuracy = acc
            best_threshold = threshold
    
    print(f"\nâœ¨ Optimal Threshold: {best_threshold:.4f}")
    
    # ========================================
    # Ã‰TAPE 7: GÃ©nÃ©rer soumissions
    # ========================================
    print_step(7, "GÃ©nÃ©ration des soumissions")
    
    # Appliquer le seuil
    y_pred_ensemble = (y_pred_avg > best_threshold).astype(int)
    y_pred_simple = (y_pred_final > best_threshold).astype(int)
    
    # ROW_ID commence Ã  527073
    row_ids = np.arange(len(X_train), len(X_train) + len(y_pred_ensemble))
    
    # 2 soumissions
    submission_ensemble = pd.DataFrame({
        'ROW_ID': row_ids,
        'prediction': y_pred_ensemble
    })
    
    submission_final = pd.DataFrame({
        'ROW_ID': row_ids,
        'prediction': y_pred_simple
    })
    
    output_dir = Path('submissions')
    output_dir.mkdir(exist_ok=True)
    
    file1 = output_dir / 'submission_ensemble_avg.csv'
    file2 = output_dir / 'submission_final_lgbm.csv'
    
    submission_ensemble.to_csv(file1, index=False)
    submission_final.to_csv(file2, index=False)
    
    print(f"âœ“ {file1}")
    print(f"âœ“ {file2}")
    
    # ========================================
    # RÃ©sumÃ©
    # ========================================
    elapsed_time = time.time() - start_time
    
    print_header("âœ… RÃ‰SUMÃ‰ FINAL")
    print(f"â±ï¸  Temps total: {elapsed_time:.1f}s ({elapsed_time/60:.1f}min)")
    print(f"ğŸ¯ Optimal Threshold: {best_threshold:.4f}")
    print(f"ğŸ“Š Val Accuracy (seuil): {best_val_accuracy:.4f}")
    print(f"ğŸš€ Soumissions gÃ©nÃ©rÃ©es: 2 fichiers")
    print(f"\nğŸ“ˆ StratÃ©gies utilisÃ©es:")
    print(f"   âœ“ Hyperparameter Tuning (2 configs)")
    print(f"   âœ“ Simple Ensemble (moyenne 3 modÃ¨les)")
    print(f"   âœ“ Threshold Optimization (0.40-0.60)")
    print(f"\nğŸ’¾ Ã€ soumettre:")
    print(f"   1ï¸âƒ£  {file1} (Ensemble - recommandÃ©)")
    print(f"   2ï¸âƒ£  {file2} (Simple LightGBM)")
    print(f"\nğŸ“ Gains estimÃ©s: +0.2-0.5% â†’ Target: 51.0%+")
    print("="*60)


if __name__ == "__main__":
    main()
