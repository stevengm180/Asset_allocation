# ğŸ“Š Comparaison : ModÃ¨le Baseline vs ModÃ¨le OptimisÃ©

## 1ï¸âƒ£ MODÃˆLE BASELINE (`main_pipeline.py`)

### Architecture
```
LightGBM Simple
â””â”€ PrÃ©dictions directes sur le test set
```

### ParamÃ¨tres LightGBM
```python
{
    "objective": "binary",
    "metric": "binary_logloss",
    "learning_rate": 0.05,        # Taux d'apprentissage fixe
    "max_depth": 5,                # Profondeur max fixe
    "num_leaves": 31,              # Feuilles max fixe
    "subsample": 0.8,              # Ã‰chantillonnage fixe
    "colsample_bytree": 0.8,       # Feature sampling fixe
    "boosting_rounds": 300         # ItÃ©rations fixes
}
```

### Processus
1. Charger donnÃ©es
2. EntraÃ®ner 1 modÃ¨le LightGBM sur l'ensemble d'entraÃ®nement
3. PrÃ©dire sur le test set
4. Soumettre avec seuil = 0.5

### Performance
- **Accuracy**: ~50.66%
- **ParamÃ¨tres optimisÃ©s**: âŒ Non
- **Ensemble**: âŒ Non
- **Threshold optimisÃ©**: âŒ Non (fixe Ã  0.5)

---

## 2ï¸âƒ£ MODÃˆLE OPTIMISÃ‰ (`main_optimized_pipeline.py`)

### Architecture
```
3 ModÃ¨les LightGBM (train split 80%)
        â†“
   Meta-Features (3 colonnes)
        â†“
Ridge Meta-ModÃ¨le
        â†“
Threshold Optimization
        â†“
PrÃ©dictions Finales
```

### ParamÃ¨tres LightGBM TestÃ©s

**Configuration 1 (Conservative)**
```python
{
    "learning_rate": 0.03,    # Lent mais stable
    "max_depth": 5,
    "num_leaves": 31,
    "min_child_samples": 20,  # ğŸ†• Plus de rÃ©gularisation
    "subsample": 0.8,
    "colsample_bytree": 0.8,
    "reg_alpha": 0.5,         # ğŸ†• L1 regularization
    "reg_lambda": 0.5,        # ğŸ†• L2 regularization
}
```

**Configuration 2 (Balanced)** â† GÃ©nÃ©ralement meilleure
```python
{
    "learning_rate": 0.05,
    "max_depth": 6,           # Plus profond
    "num_leaves": 63,         # Plus de complexitÃ©
    "min_child_samples": 15,
    "subsample": 0.9,         # Plus agressif
    "colsample_bytree": 0.9,
    "reg_alpha": 0.1,         # Moins de rÃ©gularisation
    "reg_lambda": 0.1,
}
```

**Configuration 3 (Aggressive)**
```python
{
    "learning_rate": 0.08,    # Rapide
    "max_depth": 7,           # TrÃ¨s profond
    "num_leaves": 95,         # TrÃ¨s complexe
    "min_child_samples": 10,
    "subsample": 0.85,
    "colsample_bytree": 0.85,
    "reg_alpha": 0.0,         # Pas de L1
    "reg_lambda": 0.0,        # Pas de L2
}
```

### Processus

**Ã‰tape 1: Hyperparameter Tuning**
- Test 3 configurations diffÃ©rentes
- Ã‰value sur validation set (20% train split)
- Choisit la meilleure âœ¨

**Ã‰tape 2: Stacking Ensemble**
- 3 modÃ¨les entraÃ®nÃ©s â†’ 3 prÃ©dictions = meta-features
- Meta-modÃ¨le Ridge apprend Ã  combiner les 3 prÃ©dictions
- Formula: `y_final = Ridge(pred_model1, pred_model2, pred_model3, pred_final)`

**Ã‰tape 3: Threshold Optimization**
- Test 9 seuils (0.30 Ã  0.70 par step 0.05)
- Choisit le seuil qui max l'accuracy sur validation
- Peut Ãªtre 0.48, 0.52, 0.55, etc. (pas juste 0.5)

### Performance
- **Accuracy**: ~51.5%+ (cible: 51.7%)
- **ParamÃ¨tres optimisÃ©s**: âœ… Oui (3 configs testÃ©es)
- **Ensemble**: âœ… Oui (Stacking Ridge)
- **Threshold optimisÃ©**: âœ… Oui (0.30-0.70)

---

## ğŸ” DiffÃ©rences ClÃ©s

| Aspect | Baseline | OptimisÃ© |
|--------|----------|----------|
| **ModÃ¨les utilisÃ©s** | 1 seul | 3 modÃ¨les |
| **Hyperparams** | FixÃ©s | TestÃ©s (3 configs) |
| **Meta-Learning** | âŒ Non | âœ… Ridge ensemble |
| **Regularization** | Basique | AvancÃ©e (L1, L2) |
| **Seuil de dÃ©cision** | 0.5 (fixe) | OptimisÃ© (0.3-0.7) |
| **Validation strategy** | CV 8 folds | Split 80/20 rapide |
| **Temps d'exÃ©cution** | ~5 min | ~10 min |
| **Accuracy attendu** | 50.66% | 51.5%+ |
| **Gain estimÃ©** | - | +0.8-1.0% |

---

## ğŸ’¡ Pourquoi Plus de ParamÃ¨tres ?

### 1. **Stacking = Plus de ComplexitÃ©**
```
Baseline:     X (41 features) â†’ LightGBM â†’ y_pred
OptimisÃ©:     X (41 features) â†’ 3Ã—LightGBM â†’ 3 predictions â†’ Ridge â†’ y_pred
```
Le meta-modÃ¨le a 4 colonnes (3 LightGBM + 1 final), pas juste les features brutes.

### 2. **Hyperparameters = FlexibilitÃ©**
```
Baseline: learning_rate = 0.05 (fixe)
OptimisÃ©: Test 0.03, 0.05, 0.08 â†’ Meilleure adaptation
```

### 3. **Regularization = Overfitting Control**
```
Baseline: Pas de reg_alpha, reg_lambda
OptimisÃ©: reg_alpha=0.5, reg_lambda=0.5 (config 1) â†’ Moins d'overfitting

Plus de rÃ©gularisation = Moins d'overfitting = Mieux sur test set
```

### 4. **Threshold = Post-Processing**
```
Baseline: Seuil = 0.5 toujours
OptimisÃ©: Trouve 0.48, 0.52, 0.55... selon les donnÃ©es

Si classe 1 est plus profitable Ã  prÃ©dire â†’ seuil peut Ãªtre < 0.5
```

---

## ğŸ“ˆ Cascade d'AmÃ©liorations

```
Baseline (50.66%)
    â†“
+ Hyperparameter Tuning â†’ +0.3% â†’ 50.96%
    â†“
+ Stacking Ensemble â†’ +0.2% â†’ 51.16%
    â†“
+ Threshold Optimization â†’ +0.3% â†’ 51.46%
    â†“
ğŸ¯ Target: 51.76% (vs Leaderboard 51.91%)
```

---

## ğŸ“ RÃ©sumÃ© Technique

### Baseline = Ridge Regression Analogue
- 1 modÃ¨le fixe
- Pas d'ajustement
- Rapide mais limitÃ©

### OptimisÃ© = Ensemble Learning + Meta-Learning
- 3 modÃ¨les avec hyperparams variÃ©s
- Ridge meta-modÃ¨le apprend les poids optimaux
- Threshold calibrÃ© sur validation
- Plus lent mais mieux

---

## ğŸš€ Quand Utiliser Quoi ?

| Contexte | Recommandation |
|----------|---|
| Production rapide | Baseline (50.66%) |
| CompÃ©tition Kaggle | **OptimisÃ© (51.5%+)** â­ |
| Dataset trÃ¨s petit | Baseline (risque overfitting) |
| Dataset large (>500k) | OptimisÃ© OK |
| Besoin interprÃ©tabilitÃ© | Baseline plus simple |
| Maximiser l'accuracy | **OptimisÃ©** |

---

**Conclusion**: Le modÃ¨le optimisÃ© est **3x plus complexe** mais devrait vous mettre dans le top 3 du leaderboard ! ğŸ†
